{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8c24435c",
      "metadata": {
        "id": "8c24435c"
      },
      "source": [
        "# Two-Stage OSR Runner (Colab / VS Code Colab Kernel)\n",
        "\n",
        "Pure-Python notebook cells (no `!` or `%` magics) to avoid syntax issues in VS Code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1e80f93e",
      "metadata": {
        "id": "1e80f93e",
        "outputId": "868916e4-cd02-42de-be0d-8985b278c0af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python: /usr/bin/python3\n",
            "cwd: /content\n",
            "torch: 2.10.0+cu128\n",
            "cuda available: True\n",
            "gpu count: 1\n",
            "gpu name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import os, sys, subprocess,torch\n",
        "\n",
        "print(\"python:\", sys.executable)\n",
        "print(\"cwd:\", os.getcwd())\n",
        "\n",
        "try:\n",
        "    subprocess.run([\"nvidia-smi\"], check=False)\n",
        "except FileNotFoundError:\n",
        "    print(\"nvidia-smi not found; checking torch CUDA ...\")\n",
        "try:\n",
        "    print(\"torch:\", torch.__version__)\n",
        "    print(\"cuda available:\", torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"gpu count:\", torch.cuda.device_count())\n",
        "        print(\"gpu name:\", torch.cuda.get_device_name(0))\n",
        "except Exception as e:\n",
        "    print(\"torch check failed:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7620ab0a",
      "metadata": {
        "id": "7620ab0a",
        "outputId": "c0784379-904c-4d73-ec6b-3368be7b5cb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive mounted\n"
          ]
        }
      ],
      "source": [
        "# Optional: mount Drive if running on Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print('Drive mounted')\n",
        "except Exception as e:\n",
        "    print('Drive mount skipped:', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2e965ca3",
      "metadata": {
        "id": "2e965ca3",
        "outputId": "b33dace2-0416-4b0d-e56a-28d7063d4f30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "repo root: /content/FYP-code\n",
            "has config: True\n",
            "has src: True\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os, subprocess\n",
        "\n",
        "REPO_URL = 'https://github.com/spinelessknave8/FYP_code.git'\n",
        "REPO_DIR = Path('/content/FYP-code')\n",
        "\n",
        "if not REPO_DIR.exists():\n",
        "    subprocess.check_call(['git', 'clone', REPO_URL, str(REPO_DIR)])\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "print('repo root:', Path.cwd())\n",
        "print('has config:', (Path('configs/default.yaml')).exists())\n",
        "print('has src:', (Path('src')).exists())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "eca6a1e1",
      "metadata": {
        "id": "eca6a1e1",
        "outputId": "566494a0-114d-415d-d429-ff90e845909b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Core deps already available\n",
            "torch: 2.10.0+cu128 torchvision: 0.25.0+cu128\n"
          ]
        }
      ],
      "source": [
        "import importlib, subprocess, sys\n",
        "\n",
        "def has(mod):\n",
        "    try:\n",
        "        importlib.import_module(mod)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "core = [\n",
        "    ('numpy', 'numpy<2'),\n",
        "    ('scipy', 'scipy>=1.10'),\n",
        "    ('PIL', 'pillow>=9.5'),\n",
        "    ('sklearn', 'scikit-learn>=1.2'),\n",
        "    ('matplotlib', 'matplotlib>=3.7'),\n",
        "    ('tqdm', 'tqdm>=4.65'),\n",
        "    ('yaml', 'pyyaml>=6.0'),\n",
        "]\n",
        "missing = [req for mod, req in core if not has(mod)]\n",
        "if missing:\n",
        "    print('Installing missing core deps:', missing)\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing])\n",
        "else:\n",
        "    print('Core deps already available')\n",
        "\n",
        "if not has('torch') or not has('torchvision'):\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'torch>=2.0', 'torchvision>=0.15'])\n",
        "\n",
        "import torch, torchvision\n",
        "print('torch:', torch.__version__, 'torchvision:', torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ab5d510e",
      "metadata": {
        "id": "ab5d510e",
        "outputId": "5da55d03-84e1-4d4a-deb2-a407ea0eaa3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "severstal path: /content/drive/MyDrive/datasets/severstal\n",
            "neu path: /content/drive/MyDrive/datasets/neu\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "sev = neu = None\n",
        "\n",
        "# 1) Direct expected locations\n",
        "direct_candidates = [\n",
        "    (Path(\"/content/drive/MyDrive/datasets/severstal\"), Path(\"/content/drive/MyDrive/datasets/neu\")),\n",
        "    (Path(\"/content/drive/Shareddrives/datasets/severstal\"), Path(\"/content/drive/Shareddrives/datasets/neu\")),\n",
        "]\n",
        "for s, n in direct_candidates:\n",
        "    if s.exists() and n.exists():\n",
        "        sev, neu = s, n\n",
        "        break\n",
        "\n",
        "# 2) Recursive fallback — include shortcut targets\n",
        "if sev is None:\n",
        "    search_roots = [\n",
        "        Path(\"/content/drive/MyDrive\"),\n",
        "        Path(\"/content/drive/Shareddrives\"),\n",
        "        Path(\"/content/drive/.shortcut-targets-by-id\"),  # ← shortcuts live here\n",
        "    ]\n",
        "    sev_hits, neu_hits = [], []\n",
        "    for root in search_roots:\n",
        "        if not root.exists():\n",
        "            continue\n",
        "        sev_hits.extend(root.glob(\"**/severstal\"))\n",
        "        neu_hits.extend(root.glob(\"**/neu\"))\n",
        "\n",
        "    sev_hits = [p for p in sev_hits if p.is_dir()]\n",
        "    neu_hits = [p for p in neu_hits if p.is_dir()]\n",
        "\n",
        "    if sev_hits and neu_hits:\n",
        "        sev, neu = sev_hits[0], neu_hits[0]\n",
        "\n",
        "print(\"severstal path:\", sev)\n",
        "print(\"neu path:\", neu)\n",
        "\n",
        "# 3) Debug: show what's actually inside the shortcut folder\n",
        "if sev is None or neu is None:\n",
        "    sc = Path(\"/content/drive/.shortcut-targets-by-id\")\n",
        "    if sc.exists():\n",
        "        print(\"\\nDebug: shortcut targets found:\")\n",
        "        for p in sc.iterdir():\n",
        "            print(\" \", p, \"→\", list(p.iterdir())[:5] if p.is_dir() else \"\")\n",
        "    raise RuntimeError(\n",
        "        \"Datasets not found. Check shortcut targets above and set explicit paths if needed.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "859d2de5",
      "metadata": {
        "id": "859d2de5",
        "outputId": "67ee7f1b-0535-48f0-d82b-5915387e77ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wrote configs/default.colab.yaml\n",
            "wrote configs/neu_split_a.colab.yaml\n",
            "wrote configs/neu_split_b.colab.yaml\n",
            "wrote configs/neu_split_c.colab.yaml\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# Build base Colab config\n",
        "base = yaml.safe_load(Path(\"configs/default.yaml\").read_text())\n",
        "base[\"device\"] = \"cuda\"\n",
        "base[\"severstal\"][\"data_root\"] = \"/content/drive/MyDrive/datasets/severstal\"\n",
        "base[\"severstal\"][\"train_csv\"] = \"train.csv\"\n",
        "base[\"severstal\"][\"images_dir\"] = \"train_images\"\n",
        "base[\"neu\"][\"data_root\"] = \"/content/drive/MyDrive/datasets/neu\"\n",
        "base[\"output_dir\"] = \"/content/drive/MyDrive/fyp_outputs\"\n",
        "\n",
        "Path(\"configs/default.colab.yaml\").write_text(yaml.safe_dump(base, sort_keys=False))\n",
        "print(\"wrote configs/default.colab.yaml\")\n",
        "\n",
        "# Build split-specific Colab configs (so split runs also use Drive paths)\n",
        "for split in [\"a\", \"b\", \"c\"]:\n",
        "    split_cfg = yaml.safe_load(Path(f\"configs/neu_split_{split}.yaml\").read_text())\n",
        "    merged = yaml.safe_load(yaml.safe_dump(base))  # deep copy\n",
        "    merged.update(split_cfg)  # add known/unknown classes\n",
        "    out = Path(f\"configs/neu_split_{split}.colab.yaml\")\n",
        "    out.write_text(yaml.safe_dump(merged, sort_keys=False))\n",
        "    print(\"wrote\", out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01049ca8",
      "metadata": {
        "id": "01049ca8",
        "outputId": "97589ad6-2cac-4741-efac-d883e2ed1e6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0/6] Building Colab configs...\n",
            "  wrote configs/default.colab.yaml\n",
            "  wrote configs/neu_split_a.colab.yaml\n",
            "  wrote configs/neu_split_b.colab.yaml\n",
            "  wrote configs/neu_split_c.colab.yaml\n",
            "[1/6] Running sanity checks...\n",
            "  sanity checks passed\n",
            "[2/6] Stage 1: PatchCore training/calibration\n",
            "[notebook] stage1 patchcore start: configs/default.colab.yaml\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "from src.pipelines.notebook_entrypoints import run_two_stage_stage1, run_split_pipeline\n",
        "\n",
        "t_all = time.time()\n",
        "print(\"[0/6] Building Colab configs...\")\n",
        "\n",
        "# Base Colab config\n",
        "base = yaml.safe_load(Path(\"configs/default.yaml\").read_text())\n",
        "base[\"device\"] = \"cuda\"\n",
        "base[\"severstal\"][\"data_root\"] = \"/content/drive/MyDrive/datasets/severstal\"\n",
        "base[\"severstal\"][\"train_csv\"] = \"train.csv\"\n",
        "base[\"severstal\"][\"images_dir\"] = \"train_images\"\n",
        "base[\"neu\"][\"data_root\"] = \"/content/drive/MyDrive/datasets/neu\"\n",
        "base[\"output_dir\"] = \"/content/drive/MyDrive/fyp_outputs\"\n",
        "\n",
        "Path(\"configs/default.colab.yaml\").write_text(yaml.safe_dump(base, sort_keys=False))\n",
        "print(\"  wrote configs/default.colab.yaml\")\n",
        "\n",
        "# Split Colab configs\n",
        "split_colab = []\n",
        "for s in [\"a\", \"b\", \"c\"]:\n",
        "    split_cfg = yaml.safe_load(Path(f\"configs/neu_split_{s}.yaml\").read_text())\n",
        "    merged = yaml.safe_load(yaml.safe_dump(base))  # deep copy\n",
        "    merged.update(split_cfg)\n",
        "    out = Path(f\"configs/neu_split_{s}.colab.yaml\")\n",
        "    out.write_text(yaml.safe_dump(merged, sort_keys=False))\n",
        "    split_colab.append(str(out))\n",
        "    print(\"  wrote\", out)\n",
        "\n",
        "print(\"[1/6] Running sanity checks...\")\n",
        "assert Path(\"/content/drive/MyDrive/datasets/severstal/train.csv\").exists(), \"Missing Severstal train.csv\"\n",
        "assert Path(\"/content/drive/MyDrive/datasets/severstal/train_images\").exists(), \"Missing Severstal train_images/\"\n",
        "assert Path(\"/content/drive/MyDrive/datasets/neu\").exists(), \"Missing NEU dataset folder\"\n",
        "print(\"  sanity checks passed\")\n",
        "\n",
        "print(\"[2/6] Stage 1: PatchCore training/calibration\")\n",
        "t = time.time()\n",
        "run_two_stage_stage1(\"configs/default.colab.yaml\")\n",
        "print(f\"  stage 1 done in {time.time() - t:.1f}s\")\n",
        "\n",
        "for i, split in enumerate(split_colab, start=3):\n",
        "    print(f\"[{i}/6] Split pipeline: {split}\")\n",
        "    t = time.time()\n",
        "    run_split_pipeline(split)\n",
        "    print(f\"  {split} done in {time.time() - t:.1f}s\")\n",
        "\n",
        "print(f\"[6/6] All done in {time.time() - t_all:.1f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd23e679",
      "metadata": {
        "id": "bd23e679"
      },
      "outputs": [],
      "source": [
        "# Metrics summary (Drive output path)\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "base = Path(\"/content/drive/MyDrive/fyp_outputs\")\n",
        "for split in [\"split_a\", \"split_b\", \"split_c\"]:\n",
        "    p = base / split / \"cascade\" / \"metrics.json\"\n",
        "    if not p.exists():\n",
        "        print(split, \"missing metrics:\", p)\n",
        "        continue\n",
        "    m = json.loads(p.read_text())\n",
        "    print(split, {\n",
        "        \"tpr_unknown_system\": m.get(\"tpr_unknown_system\"),\n",
        "        \"fpr_known_system\": m.get(\"fpr_known_system\"),\n",
        "        \"stage1_pass_rate_known\": m.get(\"stage1_pass_rate_known\"),\n",
        "        \"stage1_pass_rate_unknown\": m.get(\"stage1_pass_rate_unknown\"),\n",
        "    })\n",
        "\n",
        "# Aggregate + combined plots + display (pure Python, no ! magic)\n",
        "import sys, subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from pathlib import Path\n",
        "\n",
        "base = Path(\"/content/drive/MyDrive/fyp_outputs\")\n",
        "\n",
        "subprocess.check_call([sys.executable, \"-m\", \"src.pipelines.aggregate_osr\", \"--output_dir\", str(base)])\n",
        "subprocess.check_call([\n",
        "    sys.executable, \"-m\", \"src.pipelines.plot_combined_osr\",\n",
        "    \"--output_dir\", str(base),\n",
        "    \"--out_dir\", str(base / \"combined\")\n",
        "])\n",
        "\n",
        "for split in [\"split_a\", \"split_b\", \"split_c\"]:\n",
        "    for name in [\"loss_curve.png\", \"acc_curve.png\", \"roc_osr.png\", \"hist_osr.png\"]:\n",
        "        p = base / split / \"plots\" / name\n",
        "        if p.exists():\n",
        "            plt.figure(figsize=(6, 3.5))\n",
        "            plt.title(f\"{split} - {name}\")\n",
        "            plt.imshow(mpimg.imread(p))\n",
        "            plt.axis(\"off\")\n",
        "            plt.show()\n",
        "\n",
        "for name in [\"roc_combined.png\", \"mahalanobis_combined.png\"]:\n",
        "    p = base / \"combined\" / name\n",
        "    if p.exists():\n",
        "        plt.figure(figsize=(7, 4))\n",
        "        plt.title(name)\n",
        "        plt.imshow(mpimg.imread(p))\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "# Stage-1 pass/leakage and system-vs-conditional visuals (Drive output path)\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "base = Path(\"/content/drive/MyDrive/fyp_outputs\")\n",
        "splits = [\"split_a\", \"split_b\", \"split_c\"]\n",
        "\n",
        "rows = []\n",
        "for s in splits:\n",
        "    p = base / s / \"cascade\" / \"metrics.json\"\n",
        "    if not p.exists():\n",
        "        print(f\"{s}: missing {p}\")\n",
        "        continue\n",
        "    m = json.loads(p.read_text())\n",
        "    rows.append({\n",
        "        \"split\": s,\n",
        "        \"stage1_pass_rate_known\": m.get(\"stage1_pass_rate_known\", np.nan),\n",
        "        \"stage1_pass_rate_unknown\": m.get(\"stage1_pass_rate_unknown\", np.nan),\n",
        "        \"stage1_leakage_rate_known\": m.get(\"stage1_leakage_rate_known\", np.nan),\n",
        "        \"stage1_leakage_rate_unknown\": m.get(\"stage1_leakage_rate_unknown\", np.nan),\n",
        "        \"tpr_unknown_system\": m.get(\"tpr_unknown_system\", np.nan),\n",
        "        \"fpr_known_system\": m.get(\"fpr_known_system\", np.nan),\n",
        "        \"tpr_unknown_conditional\": m.get(\"tpr_unknown_conditional\", np.nan),\n",
        "        \"fpr_known_conditional\": m.get(\"fpr_known_conditional\", np.nan),\n",
        "    })\n",
        "\n",
        "if not rows:\n",
        "    raise RuntimeError(\"No cascade metrics found in /content/drive/MyDrive/fyp_outputs.\")\n",
        "\n",
        "x = np.arange(len(rows))\n",
        "labels = [r[\"split\"] for r in rows]\n",
        "w = 0.35\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(x - w/2, [r[\"stage1_pass_rate_known\"] for r in rows], width=w, label=\"known pass rate\")\n",
        "plt.bar(x + w/2, [r[\"stage1_pass_rate_unknown\"] for r in rows], width=w, label=\"unknown pass rate\")\n",
        "plt.xticks(x, labels)\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel(\"Rate\")\n",
        "plt.title(\"Stage-1 Pass Rates by Split\")\n",
        "plt.legend()\n",
        "plt.grid(axis=\"y\", alpha=0.25)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(x - w/2, [r[\"stage1_leakage_rate_known\"] for r in rows], width=w, label=\"known leakage\")\n",
        "plt.bar(x + w/2, [r[\"stage1_leakage_rate_unknown\"] for r in rows], width=w, label=\"unknown leakage\")\n",
        "plt.xticks(x, labels)\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel(\"Rate\")\n",
        "plt.title(\"Stage-1 Leakage Rates by Split\")\n",
        "plt.legend()\n",
        "plt.grid(axis=\"y\", alpha=0.25)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(labels, [r[\"tpr_unknown_system\"] for r in rows], marker=\"o\", label=\"TPR unknown (system)\")\n",
        "plt.plot(labels, [r[\"tpr_unknown_conditional\"] for r in rows], marker=\"o\", label=\"TPR unknown (conditional)\")\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel(\"Rate\")\n",
        "plt.title(\"Unknown Detection TPR: System vs Conditional\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.25)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(labels, [r[\"fpr_known_system\"] for r in rows], marker=\"o\", label=\"FPR known (system)\")\n",
        "plt.plot(labels, [r[\"fpr_known_conditional\"] for r in rows], marker=\"o\", label=\"FPR known (conditional)\")\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel(\"Rate\")\n",
        "plt.title(\"Known Rejection FPR: System vs Conditional\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.25)\n",
        "plt.show()\n",
        "\n",
        "print(\"Split-wise cascade summary:\")\n",
        "for r in rows:\n",
        "    print(\n",
        "        r[\"split\"],\n",
        "        {\n",
        "            \"pass_known\": round(r[\"stage1_pass_rate_known\"], 4),\n",
        "            \"pass_unknown\": round(r[\"stage1_pass_rate_unknown\"], 4),\n",
        "            \"leak_known\": round(r[\"stage1_leakage_rate_known\"], 4),\n",
        "            \"leak_unknown\": round(r[\"stage1_leakage_rate_unknown\"], 4),\n",
        "            \"tpr_sys\": round(r[\"tpr_unknown_system\"], 4),\n",
        "            \"fpr_sys\": round(r[\"fpr_known_system\"], 4),\n",
        "        },\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}